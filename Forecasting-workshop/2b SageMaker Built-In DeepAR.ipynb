{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike-Share Demand Forecasting 2b: SageMaker DeepAR Algorithm\n",
    "\n",
    "We'll look at 3 ways to tackle the bike-share demand forecasting problem set up previously in the data preparation notebook:\n",
    "\n",
    "1. Applying an AWS \"Managed AI\" service ([Amazon Forecast](https://aws.amazon.com/forecast/)), to tackle the scenario as a common/commodity business problem\n",
    "2. Using a SageMaker built-in algorithm ([DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)), to approach it as a common/commodity algorithm in our own data science workbench\n",
    "3. Using a custom SageMaker algorithm, to take on the core modelling as a value-added differentiator working in our data science workbench.\n",
    "\n",
    "These approaches represent different cost/control trade-offs that we might make as a business.\n",
    "\n",
    "**This notebook shows how to apply the SageMaker DeepAR built-in algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and configuration\n",
    "\n",
    "As usual we start by loading libraries, defining configuration, and connecting to AWS SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data configuration is initialised and stored in the Data Preparation notebook\n",
    "# ...We just retrieve it here:\n",
    "%store -r\n",
    "assert bucket, \"Variable `bucket` missing from IPython store\"\n",
    "\n",
    "assert data_prefix, \"Variable `data_prefix` missing from IPython store\"\n",
    "assert target_train_filename, \"Variable `target_train_filename` missing from IPython store\"\n",
    "assert target_test_filename, \"Variable `target_test_filename` missing from IPython store\"\n",
    "assert related_filename, \"Variable `related_filename` missing from IPython store\"\n",
    "\n",
    "print(f\"Reminder: You should be in region {region}\")\n",
    "\n",
    "sm_train_filename = \"train.json\"\n",
    "sm_test_filename = \"test.json\"\n",
    "sm_inference_filename = \"predict_input.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to our AWS SDKs, and initialise our access role (which may wait a little while to ensure any newly created permissions propagate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "smsession = sagemaker.Session()\n",
    "s3 = session.client(service_name=\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_role_arn = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Determine your algorithm details\n",
    "\n",
    "Choosing to use SageMaker DeepAR algorithm for this problem, we'll need to configure the algorithm and provide data in the format it's expecting.\n",
    "\n",
    "In particular, some but not all built-in algorithms have support in the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html)...\n",
    "\n",
    "```python\n",
    "# So while we can do this...\n",
    "estimator = sagemaker.KMeans(...)\n",
    "# We can't yet do this...\n",
    "estimator = sagemaker.DeepAR(...)\n",
    "```\n",
    "\n",
    "This means we'll need to provide the URL for the container image, as listed on the [SageMaker built-in algorithms common parameters doc](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html).\n",
    "\n",
    "Luckily for us, there's a nice programmatic way (below) to fetch the image path instead of copy/paste: But it's still worth checking out the Common Parameters page for details on whether the algorithm supports GPU acceleration, distributed training, various input/output formats, etc.\n",
    "\n",
    "It's also worth checking out the [DeepAR algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
    "    region,\n",
    "    \"forecasting-deepar\",\n",
    "    repo_version=\"latest\"\n",
    ")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare training data\n",
    "\n",
    "As mentioned in the common docs, and further detailed in the [DeepAR algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html), the DeepAR algorithm expects training data in the following format, delivered either in JSONLines or Parquet format:\n",
    "\n",
    "```json\n",
    "{\"start\": \"2009-11-01 00:00:00\", \"target\": [target timeseries...], \"cat\": [categorical features...], \"dynamic_feat\": [[related TS 1...], [related TS 2...]}\n",
    "{\"start\": \"2009-11-02 00:00:00\", \"target\": [target timeseries...], \"cat\": [categorical features...], \"dynamic_feat\": [[related TS 1...], [related TS 2...]}                                                                               ...\n",
    "```\n",
    "\n",
    "We'll start out by loading our generic data files from the Data Prep notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_df = pd.read_csv(f\"./data/{target_train_filename}\")\n",
    "target_test_df = pd.read_csv(f\"./data/{target_test_filename}\")\n",
    "related_df = pd.read_csv(f\"./data/{related_filename}\")\n",
    "\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, only numeric `dynamic_feat`s are allowed in DeepAR so we'll need to convert our binary related fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_df[\"holiday\"] = related_df[\"holiday\"].astype(int)\n",
    "related_df[\"workingday\"] = related_df[\"workingday\"].astype(int)\n",
    "\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we previously split **target** timeseries into separate training and test data sets, and kept **related** datasets as one big list: Which was great for Amazon Forecast.\n",
    "\n",
    "SageMaker DeepAR wants something a little different though, as documented in the [best practices](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html#deepar_best_practices):\n",
    "\n",
    "* For the `train` data channel in training, training period data only (*target* + *related*)\n",
    "* For the `test` data channel in training, whole data set (*target* + *related*)\n",
    "* For the **inference** data set, training period *target* series + whole period *related* series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_ts = target_test_df[\"timestamp\"][0]\n",
    "related_train_df = related_df[related_df[\"timestamp\"] < first_test_ts]\n",
    "related_test_df = related_df[related_df[\"timestamp\"] >= first_test_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert our training and test sets into target format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll loop through our customer types creating a record for each:\n",
    "customer_types = target_train_df[\"customer_type\"].unique()\n",
    "\n",
    "# Related timeseries are general, not per-custtype, so we can format out here:\n",
    "dynamic_feats_train = related_train_df.drop(columns=\"timestamp\")\n",
    "dynamic_feats_train = [dynamic_feats_train[col].to_list() for col in dynamic_feats_train.columns]\n",
    "dynamic_feats_test = related_test_df.drop(columns=\"timestamp\")\n",
    "dynamic_feats_test = [dynamic_feats_test[col].to_list() for col in dynamic_feats_test.columns]\n",
    "\n",
    "# Training data set (training timestamps only):\n",
    "train_lines = []\n",
    "# Test data set (training + test timestamps):\n",
    "test_lines = []\n",
    "# Inference data set (training target + full related series):\n",
    "inference_lines = []\n",
    "\n",
    "for customer_type in customer_types:\n",
    "    ctmr_target_train_df = target_train_df[target_train_df[\"customer_type\"] == customer_type]\n",
    "    target_train = ctmr_target_train_df[\"demand\"].to_list()\n",
    "    target_test = target_test_df[target_test_df[\"customer_type\"] == customer_type][\"demand\"].to_list()\n",
    "    \n",
    "    train_lines.append({\n",
    "        \"start\": ctmr_target_train_df[\"timestamp\"].iloc[0],\n",
    "        \"target\": target_train,\n",
    "        \"dynamic_feat\": dynamic_feats_train\n",
    "    })\n",
    "    test_lines.append({\n",
    "        \"start\": ctmr_target_train_df[\"timestamp\"].iloc[0],\n",
    "        \"target\": target_train + target_test,\n",
    "        \"dynamic_feat\": [\n",
    "            dynamic_feats_train[ixf] + dynamic_feats_test[ixf] for ixf in range(len(dynamic_feats_train))\n",
    "        ]\n",
    "    })\n",
    "    inference_lines.append({\n",
    "        \"start\": ctmr_target_train_df[\"timestamp\"].iloc[0],\n",
    "        \"target\": target_train,\n",
    "        \"dynamic_feat\": [\n",
    "            dynamic_feats_train[ixf] + dynamic_feats_test[ixf] for ixf in range(len(dynamic_feats_train))\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*JSON Lines* format is JSON, but with newline-separated records instead of a parent `[... , ...]` array: So a whole JSON Lines file is *not* valid JSON, but each line of the file *is*.\n",
    "\n",
    "Write our training and test files in JSON Lines, and upload to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing data sets to file...\")\n",
    "!mkdir -p ./data/smdeepar\n",
    "\n",
    "with open(f\"./data/smdeepar/{sm_train_filename}\", \"w\") as f:\n",
    "    for ix in range(len(train_lines)):\n",
    "        if (ix > 0):\n",
    "            f.write(\"\\n\")\n",
    "        f.write(json.dumps(train_lines[ix]))\n",
    "\n",
    "with open(f\"./data/smdeepar/{sm_test_filename}\", \"w\") as f:\n",
    "    for ix in range(len(test_lines)):\n",
    "        if (ix > 0):\n",
    "            f.write(\"\\n\")\n",
    "        f.write(json.dumps(test_lines[ix]))\n",
    "\n",
    "with open(f\"./data/smdeepar/{sm_inference_filename}\", \"w\") as f:\n",
    "    for ix in range(len(inference_lines)):\n",
    "        if (ix > 0):\n",
    "            f.write(\"\\n\")\n",
    "        f.write(json.dumps(inference_lines[ix]))\n",
    "\n",
    "print(\"Uploading dataframes to S3...\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/smdeepar/{sm_train_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}smdeepar/{sm_train_filename}\"\n",
    ")\n",
    "print(f\"s3://{bucket}/{data_prefix}smdeepar/{sm_train_filename}\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/smdeepar/{sm_test_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}smdeepar/{sm_test_filename}\"\n",
    ")\n",
    "print(f\"s3://{bucket}/{data_prefix}smdeepar/{sm_test_filename}\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/smdeepar/{sm_inference_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}smdeepar/{sm_inference_filename}\"\n",
    ")\n",
    "print(f\"s3://{bucket}/{data_prefix}smdeepar/{sm_inference_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up the SageMaker estimator and train the model\n",
    "\n",
    "Now the useful part - we'll be using the [Python SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to:\n",
    "\n",
    "1. Create our [Estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) defining the algorithm and fitting/hyper-parameters\n",
    "2. Define our [data channels](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig) to fit and validate on\n",
    "3. [Fit](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator.fit) a model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    # As discussed above, we need to provide the Docker image location because there's no specific\n",
    "    # `sagemaker.estimator.DeepAR` implemented (yet!):\n",
    "    image_name=training_image,\n",
    "    role=sm_role_arn,\n",
    "    # Per the docs DeepAR *can* use distributed training and GPU, but probably doesn't really need\n",
    "    # either for this configuration:\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    output_path=f\"s3://{bucket}/output/smdeepar/\",\n",
    "    base_job_name=\"bike-demo-deepar\",\n",
    "    hyperparameters={\n",
    "        \"context_length\": 24*14, # 2 weeks, same as our target forecast window\n",
    "        \"epochs\": 100,\n",
    "        \"prediction_length\": 24*14,\n",
    "        \"time_freq\": \"1H\",\n",
    "        \"early_stopping_patience\": 20,\n",
    "        \"num_eval_samples\": 24*14,\n",
    "        #\"mini_batch_size\": 128\n",
    "    },\n",
    "    train_max_run=3*60*60,\n",
    "    # SageMaker managed spot training (which can reduce training cost by up to 90%!) is as easy as\n",
    "    # requesting it when setting up the estimator:\n",
    "    #train_use_spot_instances=True,\n",
    "    #train_max_wait=5*60*60\n",
    "    # (We only avoid it here to make sure this reliably runs on-demand for large group workshops!)\n",
    ")\n",
    "\n",
    "# Training channels can be specified simply as an S3 path string, or using the s3_input API like \n",
    "# this for more control over distribution and format parameters:\n",
    "train_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{bucket}/{data_prefix}smdeepar/{sm_train_filename}\",\n",
    "    content_type=\"json\", # (The correct MIME type for JSON lines is still in community debate...)\n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "test_channel = sagemaker.session.s3_input(\n",
    "    f\"s3://{bucket}/{data_prefix}smdeepar/{sm_test_filename}\",\n",
    "    content_type=\"json\", \n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will block until training is complete, showing console output below:\n",
    "estimator.fit({ \"train\": train_channel, \"test\": test_channel })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: While the model trains...\n",
    "\n",
    "DeepAR is a deep neural model, so can take a little while to train. Why not use this time to check back on how our Amazon Forecast models are doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Kick off a Hyperparameter Optimization to improve performance\n",
    "\n",
    "The hyperparameters above are a bit of a guess, guided by the [DeepAR tuning documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html) and the characteristics of our data set.\n",
    "\n",
    "Tuning these parameters is a pain because every permutation test takes so long! Naive strategies like grid search and manual tuning get expensive fast - whether by computational costs or labour.\n",
    "\n",
    "Instead we let SageMaker's [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) take care of it: with a Bayesian optimization strategy specifically designed for these high-evaluation-cost optimization problems.\n",
    "\n",
    "Note that the [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/tuner.html)'s `fit()` method **doesn't** block by default like `Estimator`'s (because HPO jobs usually take really long times).\n",
    "\n",
    "The \"Hyperparameter Tuning Jobs\" section of the SageMaker console provides a nice UI for checking the detailed status and metrics of ongoing jobs. **You don't need to wait for this job to finish to move on to the next section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator,\n",
    "    #Alternative e.g. objective_metric_name=\"test:RMSE\",\n",
    "    objective_metric_name=\"test:mean_wQuantileLoss\",\n",
    "    hyperparameter_ranges={\n",
    "        \"mini_batch_size\": sagemaker.tuner.IntegerParameter(1, 500),\n",
    "        \"context_length\": sagemaker.tuner.IntegerParameter(10, round(24*14*1.1)),\n",
    "        \"num_cells\": sagemaker.tuner.IntegerParameter(30, 200),\n",
    "        \"num_layers\": sagemaker.tuner.IntegerParameter(1, 8),\n",
    "    },\n",
    "    objective_type=\"Minimize\",\n",
    "    # Defining the maximum number and parallelism of HPO training jobs:\n",
    "    # Note that accounts have protective limits on number of GPU instances by default.\n",
    "    # For Event Engine accounts, default max ml.p3.2xlarge = 2\n",
    "    # Set max_parallel_jobs = (limit / train_instance_count) - 1\n",
    "    # (minus one lets you run HPO and non-HPO in parallel)\n",
    "    max_jobs=# TODO: Ideally 12 or more\n",
    "    max_parallel_jobs=# TODO: Maybe only 1 for Event Engine, 2-3 if possible\n",
    "    base_tuning_job_name=\"bike-demo-deepar-tuning\"\n",
    ")\n",
    "\n",
    "tuner.fit({ \"train\": train_channel, \"test\": test_channel })\n",
    "\n",
    "# Uncomment if you like locking up your notebook for hours:\n",
    "# tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generating predictions with SageMaker Batch Transform\n",
    "\n",
    "There are two primary ways to use trained SageMaker models: [`deploy()`](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator.deploy) them as endpoints for real-time inference, or apply them to stored data sets as a batch transform.\n",
    "\n",
    "Since we already have our test scenarios stored in S3, we'll take the batch route for this example... But you might choose to set up an endpoint if, for example, providing an interactive service where users could run \"what-if\" forecasts with different weather data.\n",
    "\n",
    "In this batch transform job, SageMaker will:\n",
    "\n",
    "* Deploy our model on temporary instance(s) - similarly to how it would be deployed for a real-time endpoint\n",
    "* Read our input data from S3\n",
    "* Submit the input data to the model instance(s)\n",
    "* Collect the result data into S3\n",
    "* Clean up the temporary instances\n",
    "\n",
    "So we *don't* have to worry about deploying and deleting instances, but we *do* need to help SageMaker understand how to split input data out to model instances and how to collect the results together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count=1, # We only have 2 \"records\" (customer types) - multi-instance would be overkill!\n",
    "    instance_type=\"ml.c4.2xlarge\", # Per the docs, DeepAR only uses CPU at inference time\n",
    "    strategy=\"SingleRecord\", # Send records to the model one at a time\n",
    "    assemble_with=\"Line\", # Join results with a newline in the output file (JSONLines)\n",
    "    output_path=f\"s3://{bucket}/results/smdeepar/\",\n",
    "    env={\n",
    "        # We only want the p10, p50, p90 configs to compare with Amazon Forecast, so will override default output:\n",
    "        \"DEEPAR_INFERENCE_CONFIG\": json.dumps({\n",
    "            \"num_samples\": 100,\n",
    "            \"output_types\": [\"mean\", \"quantiles\"],\n",
    "            \"quantiles\": [\"0.1\", \"0.5\", \"0.9\"]\n",
    "        })\n",
    "    }\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    f\"s3://{bucket}/{data_prefix}smdeepar/{sm_inference_filename}\",\n",
    "    split_type=\"Line\", # Records are separated by a newline in the input file (JSONLines)\n",
    "    logs=True,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download and reformat the results\n",
    "\n",
    "Our batch transform results are stored in S3, so first let's download them to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = transformer.output_path\n",
    "\n",
    "!mkdir -p results/smdeepar\n",
    "!aws s3 cp --recursive $batch_results results/smdeepar/\n",
    "print(\"SMDeepAR results folder contents:\")\n",
    "!ls results/smdeepar/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then load the JSON lines file in to memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_local_filename = f\"results/smdeepar/{sm_inference_filename}.out\"\n",
    "results_raw = []\n",
    "with open(results_local_filename) as f:\n",
    "    for line in f:\n",
    "        results_raw.append(json.loads(line))\n",
    "\n",
    "# Note the order of records (hence the correspondence to customer_types) is preserved in SM batch:\n",
    "assert (\n",
    "    len(results_raw) == len(customer_types)\n",
    "), \"Mismatch: Batch transform should return one prediction per customer type!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and convert those predictions into the same standardized form as we did with Amazon Forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sagemaker-deepar\"\n",
    "\n",
    "first_test_ts = target_test_df[\"timestamp\"].iloc[0]\n",
    "test_start_dt = datetime(\n",
    "    int(first_test_ts[0:4]),\n",
    "    int(first_test_ts[5:7]),\n",
    "    int(first_test_ts[8:10]),\n",
    "    int(first_test_ts[11:13]),\n",
    "    int(first_test_ts[14:16]),\n",
    "    int(first_test_ts[17:])\n",
    ")\n",
    "test_end_dt = test_start_dt + timedelta(days=14)\n",
    "\n",
    "# Timestamps aren't listed in the DeepAR output, so we synthesize them from the test data:\n",
    "test_df = target_test_df.copy()\n",
    "test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\n",
    "test_df = test_df[test_df[\"timestamp\"] < test_end_dt]\n",
    "ctype_test_dfs = {\n",
    "    ctype: test_df[test_df[\"customer_type\"] == ctype]\n",
    "for ctype in customer_types}\n",
    "\n",
    "clean_results_df = pd.DataFrame()\n",
    "for ix in range(len(customer_types)):\n",
    "    prediction = results_raw[ix]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"timestamp\"] = ctype_test_dfs[customer_types[ix]][\"timestamp\"]\n",
    "    df[\"model\"] = model_id\n",
    "    df[\"customer_type\"] = customer_types[ix]\n",
    "    df[\"mean\"] = prediction[\"mean\"]\n",
    "    df[\"p10\"] = prediction[\"quantiles\"][\"0.1\"]\n",
    "    df[\"p50\"] = prediction[\"quantiles\"][\"0.5\"]\n",
    "    df[\"p90\"] = prediction[\"quantiles\"][\"0.9\"]\n",
    "\n",
    "    clean_results_df = clean_results_df.append(df)\n",
    "\n",
    "clean_results_df.to_csv(\n",
    "    f\"./results/smdeepar/results_clean.csv\",\n",
    "    index=False\n",
    ")\n",
    "print(\"Clean results saved to ./results/smdeepar/results_clean.csv\")\n",
    "clean_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Plot the performance\n",
    "\n",
    "Now that the results are in our standardized format, we can plot them using the same utility function as in the Amazon Forecast notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_plot_dt = test_end_dt - timedelta(days=21)\n",
    "actuals_plot_df = target_train_df.append(target_test_df)\n",
    "actuals_plot_df[\"timestamp\"] = pd.to_datetime(actuals_plot_df[\"timestamp\"])\n",
    "actuals_plot_df = actuals_plot_df[\n",
    "    (actuals_plot_df[\"timestamp\"] >= first_plot_dt)\n",
    "    & (actuals_plot_df[\"timestamp\"] < test_end_dt)\n",
    "]\n",
    "util.plot_fcst_results(actuals_plot_df, clean_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots should be directly comparable to the figures in the Amazon Forecast notebook.\n",
    "\n",
    "**How does \"SageMaker DeepAR\" compare to Amazon Forecast's \"DeepAR Plus\"?**\n",
    "\n",
    "With the settings we've used here, the two are likely broadly comparable. We haven't extensively optimized DeepAR's hyperparameters; neither have we utilised much of the structure of the Amazon Forecast **retail domain schema**: So there should be room for improvement on both results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Using our hyperparameter tuning job\n",
    "\n",
    "Check out the status of our hyperparameter tuning job in the \"Hyperparameter Tuning Jobs\" section of the SageMaker console: Hopefully yours is \"Completed\" like the screenshot below, though it might take up to a few hours!\n",
    "\n",
    "Note that it's possible to \"create models\" (register the model artifacts with SageMaker) and from those deploy real-time endpoints or start batch transform jobs, direct from within the console.\n",
    "\n",
    "The UI parameters correspond pretty directly to the code used earlier in this notebook, but the SDK's `fit()` and `deploy()` methods short-cut/simplify the Model and Endpoint Configuration parts of the flow.\n",
    "\n",
    "**Exercise: Using the already existing `Inference > Models` and `Inference > Batch transform jobs` as a guide, and referring to our code in steps 3 and 6, can you use the console to run your best HPO-tuned model against the same inference data set?**\n",
    "\n",
    "Note:\n",
    "\n",
    "* You want to create a batch transform job, *not* deploy an endpoint\n",
    "* Choose a different output path for the new transform job to avoid overwriting our previous model's results, but put it **in a subfolder** of the previous path, for Step 9 below to work\n",
    "\n",
    "<img src=\"BlogImages/HPOComplete.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Comparing HPO-tuned and best-guess performance\n",
    "\n",
    "If you managed the last step successfully and your training job has completed, you should have a new `.out` file somewhere in your bucket.\n",
    "\n",
    "First, let's keep copies of our previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_results_filename = results_local_filename\n",
    "untuned_clean_results_df = clean_results_df\n",
    "!mv results/smdeepar/results_clean.csv results/smdeepar/results_clean_untuned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **re-run Step 7 making these edits:**\n",
    "\n",
    "* Update the definition of `results_local_filename` to point at your new, HPO-tuned result file.\n",
    "* Update the `model_id` to `sagemaker-deepar-hpo` (adding `-hpo` on the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"I will delete this exception when I've done the above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll combine the untuned and HPO results files together, and plot the comparison as we did with Amazon Forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv results/smdeepar/results_clean.csv results/smdeepar/results_clean_hpo.csv\n",
    "\n",
    "comparison_results_df = untuned_clean_results_df.append(clean_results_df)\n",
    "\n",
    "comparison_results_df.to_csv(\n",
    "    f\"./results/smdeepar/results_clean.csv\",\n",
    "    index=False\n",
    ")\n",
    "print(\"Full results saved to ./results/smdeepar/results_clean.csv\")\n",
    "comparison_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_fcst_results(actuals_plot_df, comparison_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the updated graphs compare? Has the model qualitatively improved?\n",
    "\n",
    "What about the metrics reported in the console on your hyperparameter tuning job, and the normal training job? Do they suggest any quantitative changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension exercises and exploring further\n",
    "\n",
    "As with Amazon Forecast, both of our models' (untuned and HPO) results should now be stored in a standard format.\n",
    "\n",
    "Can you calculate RMSE and weighted quartile loss metrics to characterise the test data-set performance?\n",
    "\n",
    "What are the relative strengths and weaknesses of the Amazon Forecast and SageMaker DeepAR approaches?\n",
    "\n",
    "Does SageMaker DeepAR have the same sensitivity as Amazon Forecast to shifting timestamps by one calendar day? Or removing the `holiday` and `workingday` features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for joining in! (Clean-up time)\n",
    "\n",
    "As with Amazon Forecast, we didn't deploy any real-time predictor endpoints in this workshop but did still create some artifacts whose continued storage is (while relatively inexpensive) chargeable. Don't forget to check through all the sidebar tabs of the Amazon SageMaker console, and your S3 bucket, and consider cleaning up anything you don't want to keep!\n",
    "\n",
    "As always, remember also to stop this SageMaker notebook when no longer using it.\n",
    "\n",
    "We hope you've enjoyed this section and any others you're still working on. If you have any feedback for this workshop, please do get in touch via the GitHub or workshop facilitators!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
