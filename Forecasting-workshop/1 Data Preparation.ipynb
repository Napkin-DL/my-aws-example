{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike-Share Demand Forecasting 1: Data Preparation\n",
    "\n",
    "In this timeseries forecasting example we investigate demand in the [Capital Bikeshare scheme in 2011-12](https://www.capitalbikeshare.com/system-data), with relation to weather data.\n",
    "\n",
    "This notebook downloads the [original weather-annotated dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) from the UC Irvine website, and performs some basic preparations before uploading to S3.\n",
    "\n",
    "Later notebooks in this series fit models to the uploaded prepared data, and compare their accuracy.\n",
    "\n",
    "### 1. Dataset 소개\n",
    "\n",
    " - 멤버쉽, 렌탈 및 자전거 반납 등의 프로세스가 자동으로 이루어지는 자전거 렌탈과 관련된 자전거 공유 시스템의 데이터입니다.\n",
    " - 데이터셋은 hour.csv와 day.csv를 가지고 있으며, 아래 구성 필드로 이루어져 있습니다. [단, day.csv는 hr(시간) 필드 제외]\n",
    "    - instant: 기록된 index\n",
    "    - dteday : 대여 날짜\n",
    "    - season : 계절 데이터 (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "    - yr : 연도 (0: 2011, 1:2012)\n",
    "    - mnth : 월 데이터 ( 1 to 12)\n",
    "    - hr : 시각 (0 to 23)\n",
    "    - holiday : 휴일 여부\n",
    "    - weekday : 평일\n",
    "    - workingday : 주중이면 1, 주말 또는 휴일이면 0\n",
    "    - weathersit :\n",
    "        - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "        - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "        - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "        - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "    - temp : Normalized 온도 (Celsius), 계산식 : (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "    - atemp: Normalized 체감 온도 (Celsius), 계산식 : (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "    - hum: Normalized 습도 (The values are divided to 100 (max))\n",
    "    - windspeed: Normalized 풍속 (The values are divided to 67 (max))\n",
    "    - casual: 미동록 사용자의 대여 횟수 \n",
    "    - registered: 등록 사용자의 대여 횟수\n",
    "    - cnt: 전체 자전가 대여 횟수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and configuration<a class=\"anchor\" id=\"setup\"/>\n",
    "\n",
    "As usual we start by loading libraries, defining configuration, and connecting to AWS SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src='./BlogImages/add_img/dataset_com.png' width=700 height=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### CloudFormation으로 만들어진 S3 bucket 이름을 아래 넣습니다.\n",
    "<p><img src='./BlogImages/add_img/s3bucket.png' width=700 height=500></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'forecast-demolab-XXX'# TODO: Choose a bucket you've created, and SageMaker has full access to\n",
    "%store bucket\n",
    "\n",
    "# Data files to be stored in S3:\n",
    "data_prefix = \"data/\"\n",
    "%store data_prefix\n",
    "target_train_filename = \"target_train.csv\"\n",
    "%store target_train_filename\n",
    "target_test_filename = \"target_test.csv\"\n",
    "%store target_test_filename\n",
    "related_filename = \"related.csv\"\n",
    "%store related_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to our AWS SDKs, and initialise our access role (which may wait a little while to ensure any newly created permissions propagate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "s3 = session.client(service_name=\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch the source data<a class=\"anchor\" id=\"fetch\"/>\n",
    "\n",
    "Since this data set is comparatively small, we can process it here in the notebook instance without requiring large disk allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.zip http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "!rm -rf ./data/raw\n",
    "!mkdir -p ./data/raw\n",
    "!unzip data.zip -d ./data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UCI dataset documentation](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) tells us what to expect in terms of column headers, ranges and types, which we can load here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\n",
    "    \"./data/raw/hour.csv\",\n",
    "    index_col=\"instant\",\n",
    "    dtype={\n",
    "        \"dteday\": str,\n",
    "        \"season\": int,\n",
    "        \"yr\": int,\n",
    "        \"mnth\": int,\n",
    "        \"hr\": int,\n",
    "        \"holiday\": bool,\n",
    "        \"weekday\": int,\n",
    "        \"workingday\": bool,\n",
    "        \"weathersit\": int,\n",
    "        \"temp\": float,\n",
    "        \"atemp\": float,\n",
    "        \"hum\": float,\n",
    "        \"windspeed\": float,\n",
    "        \"casual\": int,\n",
    "        \"registered\": int,\n",
    "        \"cnt\": int\n",
    "    }\n",
    ").sort_values([\"dteday\", \"hr\"])\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but we also have a quick check of the ranges and variation to check our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Source Data Analysis<a class=\"anchor\" id=\"fetch\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will plot the overall demand across the lifetime of the full data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = raw_df.copy()\n",
    "\n",
    "# Combine day and hour into datetime column, and drop superfluous date features:\n",
    "hourly_df[\"dteday\"] = pd.to_datetime(raw_df[\"dteday\"].map(str) + \" \" + raw_df[\"hr\"].map(str) + \":00\")\n",
    "hourly_df = hourly_df.rename(columns={ \"dteday\": \"timestamp\" }).drop(columns=[\"yr\", \"mnth\", \"hr\"])\n",
    "\n",
    "daterange = pd.date_range(\n",
    "    start=list(hourly_df[\"timestamp\"])[0],\n",
    "    end=list(hourly_df[\"timestamp\"])[-1],\n",
    "    freq='H'\n",
    ")\n",
    "tmp = pd.DataFrame({ \"timestamp\": daterange })\n",
    "tmp[\"dteday\"] = tmp[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "fill_df = tmp.merge(\n",
    "    hourly_df,\n",
    "    how=\"left\",\n",
    "    on=\"timestamp\"\n",
    ").drop(columns=[\"dteday\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "fill_df.plot(x=\"timestamp\", y=\"registered\", ax=ax, label=\"Registered Customers\")\n",
    "fill_df.plot(x=\"timestamp\", y=\"casual\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - Whole Data Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we see from the above that:\n",
    "\n",
    "1. 등록된 고객들이 대부분의 demand를 차지합니다.\n",
    "2. 비록 여름/겨울 계절성을 데이터 내에서 볼 수 있지만, 서비스의 전체 증가 및 인기도는 2년 사이 전반적으로 성장하는 것을 볼 수 있습니다.\n",
    "3. 전반적인 데이터에서 볼 수 있는 것 보다 단기간 내 강한 주기성 (spikiness)가 있습니다.\n",
    "\n",
    "** 2번은 forecast의 정확도 관점에서 어려운 상황이긴 합니다. **\n",
    "- 단지 2년의 데이터만을 가지고 있으며 연간 seasonaility가 중요하지만, 일반적으로는 중요 패턴에 대해서는 5배 더 긴 주기를 가는 과거 이력이 필요합니다.\n",
    "- 서비스의 전반적인 수요는 non-stationary 한 것으로 판단되며, 이는 많은 forecasting 방법에서 정확도를 떨어뜨릴 수 있습니다.[here](https://cs.nyu.edu/~mohri/talks/NIPSTutorial2016.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 그래프에서는 수요에 대해 요일과 시간의 영향이 명확히 볼 수 있으며, 적어도 여름인 8월에 casual 라이더의 경우 주 보다 주말의 여행 비율이 높다는 것을 알 수 있습니다.\n",
    "- 짧은 timescales에서는 데이터는 더욱 stationary 한 것을 볼 수 있고, 우리는 더욱 robust한 forecast 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_df = fill_df[(fill_df[\"timestamp\"] >= \"2012-08-01\") & (fill_df[\"timestamp\"] < \"2012-09-01\")]\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "period_df.plot(x=\"timestamp\", y=\"registered\", ax=ax, label=\"Registered Customers\")\n",
    "period_df.plot(x=\"timestamp\", y=\"casual\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - August 2012\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Trends and Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=11,ncols=2)\n",
    "fig.set_size_inches(20, 60)\n",
    "sn.boxplot(data=raw_df,y=\"casual\",orient=\"v\",ax=axes[0][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",orient=\"v\",ax=axes[0][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"season\",orient=\"v\",ax=axes[1][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"season\",orient=\"v\",ax=axes[1][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"mnth\",orient=\"v\",ax=axes[2][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"mnth\",orient=\"v\",ax=axes[2][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"hr\",orient=\"v\",ax=axes[3][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"hr\",orient=\"v\",ax=axes[3][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"weekday\",orient=\"v\",ax=axes[4][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"weekday\",orient=\"v\",ax=axes[4][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"workingday\",orient=\"v\",ax=axes[5][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"workingday\",orient=\"v\",ax=axes[5][1])\n",
    "\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"weathersit\",orient=\"v\",ax=axes[6][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"weathersit\",orient=\"v\",ax=axes[6][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"temp\",orient=\"v\",ax=axes[7][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"temp\",orient=\"v\",ax=axes[7][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"atemp\",orient=\"v\",ax=axes[8][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"atemp\",orient=\"v\",ax=axes[8][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"hum\",orient=\"v\",ax=axes[9][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"hum\",orient=\"v\",ax=axes[9][1])\n",
    "sn.boxplot(data=raw_df,y=\"casual\",x=\"windspeed\",orient=\"v\",ax=axes[10][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",x=\"windspeed\",orient=\"v\",ax=axes[10][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Casual',title=\"Box Plot On Casual\")\n",
    "axes[0][1].set(ylabel='Registered',title=\"Box Plot On Registered\")\n",
    "axes[1][0].set(xlabel='Season', ylabel='Casual',title=\"Box Plot On Casual Across Season\")\n",
    "axes[1][1].set(xlabel='Season', ylabel='Registered',title=\"Box Plot On Registered Across Season\")\n",
    "axes[2][0].set(xlabel='Months', ylabel='Casual',title=\"Box Plot On Casual Across Months\")\n",
    "axes[2][1].set(xlabel='Months', ylabel='Registered',title=\"Box Plot On Registered Across Months\")\n",
    "axes[3][0].set(xlabel='Hours', ylabel='Casual',title=\"Box Plot On Casual Across Hour Of The Day\")\n",
    "axes[3][1].set(xlabel='Hours', ylabel='Registered',title=\"Box Plot On Registered Across Hour Of The Day\")\n",
    "axes[4][0].set(xlabel='Weekday', ylabel='Casual',title=\"Box Plot On Casual Across Weekday\")\n",
    "axes[4][1].set(xlabel='Weekday', ylabel='Registered',title=\"Box Plot On Registered Across Weekday\")\n",
    "axes[5][0].set(xlabel='Working Day', ylabel='Casual',title=\"Box Plot On Casual Across Working Day\")\n",
    "axes[5][1].set(xlabel='Working Day', ylabel='Registered',title=\"Box Plot On Registered Across Working Day\")\n",
    "\n",
    "axes[6][0].set(xlabel='Weathersit', ylabel='Casual',title=\"Box Plot On Casual Across Weathersit\")\n",
    "axes[6][1].set(xlabel='Weathersit', ylabel='Registered',title=\"Box Plot On Registered Across Weathersit\")\n",
    "axes[7][0].set(xlabel='Temp', ylabel='Casual',title=\"Box Plot On Casual Across Temp\")\n",
    "axes[7][1].set(xlabel='Temp', ylabel='Registered',title=\"Box Plot On Registered Across Temp\")\n",
    "axes[8][0].set(xlabel='aTemp', ylabel='Casual',title=\"Box Plot On Casual Across aTemp\")\n",
    "axes[8][1].set(xlabel='aTemp', ylabel='Registered',title=\"Box Plot On Registered Across aTemp\")\n",
    "axes[9][0].set(xlabel='Hum', ylabel='Casual',title=\"Box Plot On Casual Across Hum\")\n",
    "axes[9][1].set(xlabel='Hum', ylabel='Registered',title=\"Box Plot On Registered Across Hum\")\n",
    "axes[10][0].set(xlabel='Windspeed', ylabel='Casual',title=\"Box Plot On Casual Across Windspeed\")\n",
    "axes[10][1].set(xlabel='Windspeed', ylabel='Registered',title=\"Box Plot On Registered Across Windspeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(fill_df,figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 feature 대비 종속 변수가 어떻게 영향을 받는지 확인하기 위해 아래 상관 행렬을 만들어 봅니다.\n",
    "\n",
    "   - 종속변수 ```cnt``` 대비하여 ```temp/atemp```는 양의 상관관계를 가지며, ```hum```은 음의 상관관계를 갖는 것을 확인할 수 있습니다.\n",
    "   - ```windspeed```는 상관성이 낮은 것으로 판단됩니다.\n",
    "   - ```atemp```는 ```temp```와 상호 높은 상관 관계를 가지므로 둘 중 하나의 데이터는 삭제하는 것이 좋습니다.  \n",
    "     *multicollinearity 문제 : 독립변수 간의 상관관계가 발생할 경우 삭제하는 것이 좋음*\n",
    "   - ```casual```과 ```registered```는 종속변수 ```cnt```를 분리한 값이므로 이후 모델링 시에는 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = fill_df[[\"temp\",\"atemp\",\"casual\",\"registered\",\"hum\",\"windspeed\",\"cnt\"]].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load data & interpolate gaps<a class=\"anchor\" id=\"convert\"/>\n",
    "\n",
    "The raw dataset has:\n",
    "\n",
    "* 날짜 / 시간 열 중복 제거 필요\n",
    "* 대여 횟수가 없는 시간은 raw dataset에서 나타나지 않음\n",
    "\n",
    "...which we'll deal with here, starting with the timestamp tidy-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = raw_df.copy()\n",
    "\n",
    "# Combine day and hour into datetime column, and drop superfluous date features:\n",
    "hourly_df[\"dteday\"] = pd.to_datetime(raw_df[\"dteday\"].map(str) + \" \" + raw_df[\"hr\"].map(str) + \":00\")\n",
    "hourly_df = hourly_df.rename(columns={ \"dteday\": \"timestamp\" }).drop(columns=[\"yr\", \"mnth\", \"hr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무도 자전거를 사용하지 않은 시간 동안 행은 누락되었지만 적용 범위 내 매일 최소 하나의 행은 존재합니다.\n",
    "결측치는 아래와 같이 처리하였습니다.\n",
    "\n",
    " 1. 라이더 수 (casual, registered, cnt) 는 0으로 설정 (cnt=0 은 행 대체가 적용된 경우 복구 시 사용)\n",
    " 2. 일별 값 (season, holiday, weekday, workingday)은 해당 날짜의 원본 레코드의 평균값을 사용\n",
    " 3. 날씨 데이터 (weathersit, temp, atemp, hum, windspeed): 시간 상 현재 레코드들과 가장 근접한 값들로 보간\n",
    "\n",
    "날씨 데이터는 유일한 대체이며 시간별 보간으로 시간별 날씨 데이터의 결측 시간을 채우게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the full range of hours covered by the data-set, to the number of records:\n",
    "daterange = pd.date_range(\n",
    "    start=list(hourly_df[\"timestamp\"])[0],\n",
    "    end=list(hourly_df[\"timestamp\"])[-1],\n",
    "    freq='H'\n",
    ")\n",
    "\n",
    "n_raw_records = len(hourly_df)\n",
    "n_range_hours = len(daterange)\n",
    "print(f\"{n_raw_records} raw records vs {n_range_hours} hours in date range\")\n",
    "if (n_raw_records == n_range_hours):\n",
    "    print(\"Data fully specified\")\n",
    "elif (n_raw_records < n_range_hours):\n",
    "    # (We expect to see this)\n",
    "    print(\"MISMATCH: Missing data will be interpolated\")\n",
    "elif (n_raw_records > n_range_hours):\n",
    "    raise NotImplementedError(\"This script can't deal with duplicates yet!\")\n",
    "\n",
    "# Construct a fully range-covering table\n",
    "# (including day-granularity fields taken from aggregating the source table)\n",
    "tmp = pd.DataFrame({ \"timestamp\": daterange })\n",
    "tmp[\"dteday\"] = tmp[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "fill_df = tmp.merge(\n",
    "    raw_df.groupby(\"dteday\").agg(\"mean\")[[\"season\", \"holiday\", \"weekday\", \"workingday\"]],\n",
    "    how=\"left\",\n",
    "    on=\"dteday\"\n",
    ").drop(columns=[\"dteday\"])\n",
    "\n",
    "# Join the whole-range table to our target, and fill in the day-granularity fields\n",
    "assert (\n",
    "    hourly_df.isna().sum().sum() == 0\n",
    "), \"These imputations assume no missing values in source data set records!\"\n",
    "\n",
    "imputed_df = fill_df.merge(hourly_df, how=\"left\", on=\"timestamp\", suffixes=(\"_day\", \"\"))\n",
    "imputed_df[\"season\"].fillna(imputed_df[\"season_day\"], inplace=True)\n",
    "imputed_df[\"holiday\"].fillna(imputed_df[\"holiday_day\"], inplace=True)\n",
    "imputed_df[\"weekday\"].fillna(imputed_df[\"weekday_day\"], inplace=True)\n",
    "imputed_df[\"workingday\"].fillna(imputed_df[\"workingday_day\"], inplace=True)\n",
    "imputed_df.drop(columns=[\"season_day\", \"holiday_day\", \"weekday_day\", \"workingday_day\"], inplace=True)\n",
    "\n",
    "# Fill all missing demand values with zero (which is why the records were missing in the first place)\n",
    "imputed_df[\"casual\"].fillna(0, inplace=True)\n",
    "imputed_df[\"registered\"].fillna(0, inplace=True)\n",
    "imputed_df[\"cnt\"].fillna(0, inplace=True)\n",
    "\n",
    "# Interpolate over time for missing weather data fields:\n",
    "imputed_df = imputed_df.set_index(\"timestamp\")\n",
    "imputed_df[\"weathersit\"] = imputed_df[\"weathersit\"].interpolate(method=\"time\").round()\n",
    "imputed_df[\"temp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"atemp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"hum\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"windspeed\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df = imputed_df.reset_index()\n",
    "\n",
    "assert not imputed_df.isna().any().any(), \"Imputed DF should not have any remaining nulls!\"\n",
    "assert len(imputed_df) == n_range_hours, \"Imputed DF should fully cover time range!\"\n",
    "\n",
    "print(\"Imputation complete\")\n",
    "imputed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to extend feature engineering here, just leaving timestamp and *_demand columns alone:\n",
    "target_suffix = \"_demand\"\n",
    "full_df = imputed_df.rename(columns={\n",
    "    \"casual\": f\"casual{target_suffix}\",\n",
    "    \"registered\": f\"registered{target_suffix}\",\n",
    "    \"cnt\": f\"total{target_suffix}\"\n",
    "})\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/test split for eodel evaluation<a class=\"anchor\" id=\"split\"/>\n",
    "\n",
    "Since a lot of feature engineering has been done for us already, the main outstanding question is how we'll evaluate the accuracy of our models.\n",
    "\n",
    "### How to evaluate forecast models\n",
    "\n",
    "forecasting 알고리즘을 평가하는 것과 일반적인 ML 방법과의 중요한 차이는 **casuality (인과관계)** 입니다. 과거 데이터 기반으로 우리의 모델을 fitting 해야 합니다. 그리고, 알수 없는 \"미래\" 데이터를 평가합니다. 모델의 fitting 프로세스에서는 미래의 데이터가 보이지 않도록 합니다.\n",
    "\n",
    "**[Backtesting](https://en.wikipedia.org/wiki/Backtesting)** (or \"hindcasting\")는 이런 인관관계를 평가하는 프로세스입니다. 과거 데이터에서 시점 기준으로 하나 이상의 포인트를 선택하는 방법으로 cut-off 시나리오에서 사용합니다. 포인트 이전 데이터는 학습 데이터로 사용하고, 이후 데이터는 평가용으로 사용합니다.  \n",
    "\n",
    "<img src=\"BlogImages/backtest.png\"/>\n",
    "\n",
    "다중의 timeseries를 갖는 상황에서, Target Timeseries와 Related Timeseries의 차이점을 아래 그림으로 알 수 있습니다.\n",
    "\n",
    "* **target timeseries** (the things we want the model to forecast), and\n",
    "* **[related timeseries](https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html)** (which we know ahead of time for our forecast window)\n",
    "\n",
    "Therefore our train/test split will apply to the target timeseries only.\n",
    "\n",
    "<img src=\"BlogImages/rts_viz.png\">\n",
    "\n",
    "\n",
    "### Choosing the Setup for our Bike Forecasting example\n",
    "\n",
    "소스 데이터는 **hourly granularity**를 가지고 있으며, 이는 예측 가능한 단위의 가장 하한 값입니다.\n",
    "이 granularity를 사용하거나 또는 aggregate up할지를 선택하는 것은 적절한 **forecast horizon**가 무엇인지를 가이드할 것입니다.\n",
    "\n",
    "* 모델 학습은 주어진 forecast horizon에 대해 전체 정확도를 최적화할 것입니다. 따라서, 대부분의 유용한 horizon은 **비즈니스 문제**에 따라 선정해야 합니다. 예를 들어, 최선을 다해 1년을 예측하는 모델을 학습했다고 하지만, 실제 1개월 계획을 세우기 위해 사용하는 경우에는 성능이 떨어질 수 있습니다.\n",
    "* DeepAR과 같은 RNN (Recurrent Neural Models)과도 비교할 것이기 때문에, granularity가 적어도 forecast window의 특정 비율일 때 이 아키텍처들의 일부에서 나쁘게 작동한다는 점도 고려할 필요가 있습니다. 어떤 패턴의 주기를 학습 할 수 있는지에 대한 제약 사항은 샘플 주기에 비례합니다. 특히, Amazon Forecast는 최대 500개의 샘플 forecast horizon까지 가능합니다. [limit](https://docs.aws.amazon.com/forecast/latest/dg/limits.html)\n",
    "* 알고리즘은 context window 내 데이터가 거의 **stationary**하고 가장 긴 fluctuations의 여러 사이클을 잡아낼 때 최상으로 수행될 것입니다. 분기 또는 연간 forecasts는 사용 가능한 데이터셋에 대해 적합하지 않을 수 있습니다.\n",
    "\n",
    "또한,\n",
    "\n",
    "* (적어도 일부 국가에서) 괜찮은 날씨 예측이 가능한 기간과 수요에 적합하도록 bikes의 이동 및 배치를 위해 적합한 사전 planning 단계를 수행할 수 있는 기간 사이의 대략적인 평균을 고려하고,\n",
    "* 시간별 샘플이 유지될 정도로 짧은 기간이며, 집계가 적절한 경우를 고려하여, \n",
    "\n",
    "이번에는 **14일 (2주) horizon**에 대한 predictors를 학습할 것입니다.\n",
    "\n",
    "연말 기간이 business 적으로 중요한 시간임으로 가정하여, 2012-12-01 까지를 최종 테스트 cutoff로 offset합니다. final evaluation window는 단지 2년의 데이터를 제공하는 상황이기 때문에 모델에 대해 연간 seasonality의 노출을 최대화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='BlogImages/add_img/dataset.png' width=900 height=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test_start = \"2012-12-01\"\n",
    "\n",
    "full_headers = full_df.columns.to_list()\n",
    "timestamp_header = \"timestamp\"\n",
    "target_headers = list(filter(lambda s: s.endswith(target_suffix), full_headers))\n",
    "target_nontotal_headers = list(filter(lambda s: s != \"total_demand\", target_headers))\n",
    "related_headers = list(filter(lambda s: (s not in target_headers) and (s != timestamp_header), full_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivoted dataframe of target variables, still sorted by timestamp:\n",
    "target_full_df = full_df[[timestamp_header] + target_nontotal_headers].melt(\n",
    "    id_vars=[timestamp_header],\n",
    "    value_vars=target_nontotal_headers,\n",
    "    var_name=\"customer_type\",\n",
    "    value_name=\"demand\"\n",
    ").sort_values(by=[timestamp_header, \"customer_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Strip \"_demand\" from the target IDs:\n",
    "target_full_df[\"customer_type\"] = target_full_df[\"customer_type\"].apply(lambda s: s[0:-len(target_suffix)])\n",
    "\n",
    "target_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_df = target_full_df[target_full_df[timestamp_header] < cutoff_test_start]\n",
    "target_test_df = target_full_df[target_full_df[timestamp_header] >= cutoff_test_start]\n",
    "\n",
    "related_df = full_df[[timestamp_header] + related_headers]\n",
    "\n",
    "print(f\"{len(target_train_df)} target training points\")\n",
    "print(f\"{len(target_test_df)} target test points\")\n",
    "print(f\"{len(related_df)} related timeseries points\")\n",
    "\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing dataframes to file...\")\n",
    "target_train_df.to_csv(\n",
    "    f\"./data/{target_train_filename}\",\n",
    "    index=False\n",
    ")\n",
    "target_test_df.to_csv(\n",
    "    f\"./data/{target_test_filename}\",\n",
    "    index=False\n",
    ")\n",
    "related_df.to_csv(\n",
    "    f\"./data/{related_filename}\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Uploading dataframes to S3...\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_train_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_train_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_test_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_test_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{related_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{related_filename}\"\n",
    ")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is uploaded to S3, we can train models and compare their performance! Move on to the next notebook to start fitting predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
