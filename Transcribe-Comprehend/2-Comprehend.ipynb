{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Amazon Comprehend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/comprehend.png\">\n",
    "\n",
    "Amazon Comprehend는 Natural Language Processing (NLP) 를 사용하여 문서들의 콘텐츠에서 insights 를 추출하는 서비스입니다.\n",
    "\n",
    "-----------------------------------\n",
    "> 1. **Input** : UTF-8 포맷의 text 파일\n",
    "> 2. **API** : JAVA, .NET, Python\n",
    "> 3. **제공 기능** (언어 별로 미제공 기능 있음)\n",
    ">    - **Entities, Key phrases, Language, Sentiments, Topic Modeling**\n",
    ">    - **Syntax , Custom Classification (한국어 미지원), Custom Entity Recognition (영어만 가능)**  \n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# External Dependencies:\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...We just retrieve it here:\n",
    "%store -r\n",
    "assert bucket_name, \"Variable `bucket_name` missing from IPython store\"\n",
    "print(bucket_name)\n",
    "\n",
    "## bucket_name ='transcribe-comprehend-demo-test-XXX'  ## CloudFormation의 ouput에서 나온 S3Bucket\n",
    "\n",
    "assert transcript, \"Variable `transcript` missing from IPython store\"\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the Dominant Language\n",
    "-----------------------------------\n",
    "- **Language (한국어 지원)** : 주요 언어명, 100여개 식별 가능 (한국어 지원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Calling DetectDominantLanguage')\n",
    "res = comprehend.detect_dominant_language(Text = transcript)\n",
    "result = res['Languages'][0]\n",
    "LanguageCode = result['LanguageCode']\n",
    "print(\"Language : {}, Score : {} \\n\\n\".format(LanguageCode, result['Score']))\n",
    "\n",
    "print(\"[detail result] \\n\" + json.dumps(res, sort_keys=True, indent=4))\n",
    "\n",
    "print(\"End of DetectDominantLanguage\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Named Entities \n",
    "-----------------------------------\n",
    "- **Entities (한국어 지원)** : Named Entitiy Recognition 수행\n",
    "<img src=\"./images/NER항목.png\" width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calling DetectEntities')\n",
    "res = comprehend.detect_entities(Text=transcript, LanguageCode=LanguageCode)\n",
    "list_result =[]\n",
    "for result in res['Entities']:\n",
    "    list_result.append([result['Text'], result['Type'], result['Score'], result['BeginOffset'], result['EndOffset']])\n",
    "df = pd.DataFrame(list_result, columns=['Text', 'Type', 'Score','BeginOffset', 'EndOffset'])\n",
    "df=df.sort_values(by='Score', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Key Phrases\n",
    "-----------------------------------\n",
    "- **Key phrases (한국어 지원)** : 문서 내 키워드 추출\n",
    "   - 특정 사물을 설명하는 명사구를 포함하는 문자열 의미\n",
    "   - 명사구(관사+형용사+명사)와 신뢰 수준을 제공\n",
    "   - 모든 문서는 동일 언어로 작성되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calling DetectKeyPhrases')\n",
    "res = comprehend.detect_key_phrases(Text=transcript, LanguageCode=LanguageCode)\n",
    "list_result =[]\n",
    "for result in res['KeyPhrases']:\n",
    "    list_result.append([result['Text'], result['Score'], result['BeginOffset'], result['EndOffset']])\n",
    "df = pd.DataFrame(list_result, columns=['Text', 'Score','BeginOffset', 'EndOffset'])\n",
    "df = df.sort_values(by='Score', ascending=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Sentiment\n",
    "-----------------------------------\n",
    "- **Sentiments (한국어 지원)** : 긍정, 부정, 중립, 혼합 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calling DetectSentiment')\n",
    "\n",
    "res = comprehend.detect_sentiment(Text=transcript, LanguageCode=LanguageCode)\n",
    "\n",
    "print(\"Sentiment : {}  \\n Positive : {:0.5f} \\n Negative : {:0.5f} \\n Neutral : {:0.5f} \\n Mixed : {:0.5f} \\n \".format(\n",
    "res['Sentiment'], res['SentimentScore']['Positive'], res['SentimentScore']['Negative'], res['SentimentScore']['Neutral'], res['SentimentScore']['Mixed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Syntax\n",
    "-----------------------------------\n",
    "- **Syntax (한국어 미지원)**\n",
    "   - 17개의 Part-of-Speech (PoS) 식별\n",
    "   - ADJ (형용사),ADP (전치사/후치사),ADV (부사),AUX (조동사),NOUN (명사),NUM 등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Calling DetectSyntax')\n",
    "\n",
    "# res = comprehend.detect_syntax(Text=transcript, LanguageCode=LanguageCode)\n",
    "# list_result =[]\n",
    "# for result in res['SyntaxTokens']:\n",
    "#     list_result.append([result['Text'], result['TokenId'],result['PartOfSpeech']['Tag'] ,result['PartOfSpeech']['Score'], result['BeginOffset'], result['EndOffset']])\n",
    "# df = pd.DataFrame(list_result, columns=['Text', 'TokenId', 'Tag', 'Score', 'BeginOffset', 'EndOffset'])\n",
    "# df = df.sort_values(by='TokenId')\n",
    "# df \n",
    "\n",
    "text = \"It is raining today in Seattle\"\n",
    "print('Calling DetectSyntax')\n",
    "print(json.dumps(comprehend.detect_syntax(Text=text, LanguageCode='en'), sort_keys=True,\n",
    " indent=4))\n",
    "print('End of DetectSyntax\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "-----------------------------------\n",
    "- **Topic Modeling**  \n",
    "   - 문서 집합에 대한 공통 테마 결정\n",
    "   - 정치, 스포츠, 엔터테인먼트 등의 주제로 결정\n",
    "   - 문서 내 텍스트에 대한 별도 주석이 필요 없음\n",
    "   - LDA(Latent Dirichlet Allocation) 기반 학습 모델\n",
    "   - 좋은 결과를 얻기 위해서는, \n",
    "       - 최소 1,000개 문서 사용\n",
    "       - 각 문서 길이는 3문장 이상 필요\n",
    "       - 문서가 주로 숫자 데이터 위주이면 Corpus에서 제거 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobName = 'XXXXXXX' ## 작업명\n",
    "topic_modeling_prefix = 'XXXXXXX'  ## S3 내 topic modeling을 위한 documents 저장 위치\n",
    "topic_modeling_output = 'XXXXXXX'  ## S3 내 topic modeling 결과 위치 \n",
    "\n",
    "input_s3_url =\"s3://{}/{}\".format(bucket_name, topic_modeling_prefix)\n",
    "input_doc_format = \"ONE_DOC_PER_FILE\" ## \n",
    "output_s3_url = \"s3://{}/{}\".format(bucket_name, topic_modeling_output)\n",
    "data_access_role_arn = \"arn:aws:iam::XXXXXXXXXX:role/service-role/XXXXXXXXX-DataAccessRole-XXXXXXXXXXX\"  ## CloudFormation의 ouput에서 나온 DataAccessRoleArn\n",
    "number_of_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(\n",
    "    JobName=JobName,\n",
    "    NumberOfTopics=number_of_topics,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn)\n",
    "\n",
    "print('start_topics_detection_job_result: ' + json.dumps(start_topics_detection_job_result))\n",
    "while True:\n",
    "    status = comprehend.list_topics_detection_jobs(\n",
    "        Filter={\n",
    "            'JobName': JobName\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if status['TopicsDetectionJobPropertiesList'][0]['JobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    print(\"Not ready yet...\")\n",
    "    time.sleep(5)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    "print('job_id: ' + job_id)\n",
    "\n",
    "def json_default(value): \n",
    "    import datetime, json\n",
    "    if isinstance(value, datetime.date): \n",
    "        return value.strftime('%Y-%m-%d') \n",
    "    raise TypeError('not JSON serializable')\n",
    "    \n",
    "    \n",
    "describe_topics_detection_job_result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "print('describe_topics_detection_job_result: ' + json.dumps(describe_topics_detection_job_result, \n",
    "                                                            default=json_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = describe_topics_detection_job_result['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "tmp=res.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = tmp[2]\n",
    "output_filename = tmp[3] +\"/\" + tmp[4] +\"/\" + tmp[5]+\"/\" + tmp[6]\n",
    "output_path = './output/' + tmp[6]\n",
    "print(\"bucket : {}, output_path : {}\".format(bucket, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Object(bucket, output_filename).download_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = tarfile.open(output_path)\n",
    "ap.extractall('./output/topic')\n",
    "ap.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./output/topic/topic-terms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./output/topic/doc-topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
