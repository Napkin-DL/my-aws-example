## Pytorch Distributed Training Code

Generative Tensorial Reinforcement Learning (GENTRL) model ([GENTRL](https://github.com/insilicomedicine/GENTRL))에서 구현된 코드를 기반으로 AWS의 instance에서 분산학습이 가능한 Pytorch의 분산 코드를 추가하였습니다.

이후 SageMaker에서 분산이 가능한 코드는 업데이트할 예정입니다.

코드의 개발환경과 데이터셋은 기존 github 자료를 이용하시기 바랍니다. (https://github.com/insilicomedicine/GENTRL)

### Reference

- [(ADVANCED) PYTORCH 1.0 DISTRIBUTED TRAINER WITH AMAZON AWS](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html)
- [Distributed TensorFlow training using Kubeflow on Amazon EKS](https://aws.amazon.com/ko/blogs/opensource/distributed-tensorflow-training-using-kubeflow-on-amazon-eks/)
- [Scalable deep learning training using multi-node parallel jobs with AWS Batch and Amazon FSx for Lustre](https://aws.amazon.com/ko/blogs/compute/scalable-deep-learning-training-using-multi-node-parallel-jobs-with-aws-batch-and-amazon-fsx-for-lustre/)
- [Launching TensorFlow distributed training easily with Horovod or Parameter Servers in Amazon SageMaker](https://aws.amazon.com/ko/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/)
- [Distributed GPU Training](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-eks-tutorials-distributed-gpu-training.html#deep-learning-containers-eks-tutorials-distributed-gpu-training-pytorch)
- [Running distributed TensorFlow training with Amazon SageMaker](https://aws.amazon.com/ko/blogs/machine-learning/running-distributed-tensorflow-training-with-amazon-sagemaker/)
- [Distributed training a DIY AWS SageMaker model](https://godatadriven.com/blog/distributed-training-a-diy-aws-sagemaker-model/)
- [Train and Deploy the Mighty BERT based NLP models using FastBert and Amazon SageMaker](https://medium.com/@kaushaltrivedi/train-and-deploy-mighty-transformer-nlp-models-using-fastbert-and-aws-sagemaker-cc4303c51cf3)
